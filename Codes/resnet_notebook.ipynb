{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet++ Notebook\n",
    "\n",
    "This notebook separates the major code sections so you can run each step independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Basic imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "# Run this cell first to verify imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Device setup and constants\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, base_path, protocol_path, transform=None, sr=16000):\n",
    "        self.transform = transform\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        self.sr = sr\n",
    "        self.base_path = base_path  # Store base_path as instance variable\n",
    "        \n",
    "        print(f\"Loading dataset from: {base_path}\")\n",
    "        print(f\"Using protocol file: {protocol_path}\")\n",
    "        \n",
    "        if not os.path.exists(base_path):\n",
    "            raise FileNotFoundError(f\"Dataset path does not exist: {base_path}\")\n",
    "        if not os.path.exists(protocol_path):\n",
    "            raise FileNotFoundError(f\"Protocol file does not exist: {protocol_path}\")\n",
    "            \n",
    "        # Load protocol file first\n",
    "        label_dict = {}\n",
    "        with open(protocol_path, 'r') as f:\n",
    "            for line in f:\n",
    "                # Split on whitespace but keep filename as first element\n",
    "                parts = line.strip().split(maxsplit=2)\n",
    "                if len(parts) < 2:\n",
    "                    print(f\"Warning: Malformed line in protocol file: {line}\")\n",
    "                    continue\n",
    "                    \n",
    "                filename = parts[0].strip()  # Get filename without leading/trailing whitespace\n",
    "                label_type = parts[1].strip()  # Get label type (genuine or spoof)\n",
    "                label_dict[filename] = 1 if label_type == 'genuine' else 0\n",
    "                \n",
    "        print(f\"Loaded {len(label_dict)} entries from protocol file\")\n",
    "        print(\"First few protocol entries:\")\n",
    "        for i, (k, v) in enumerate(label_dict.items()):\n",
    "            if i < 5:\n",
    "                print(f\"{k}: {'genuine' if v==1 else 'spoof'}\")\n",
    "        \n",
    "        # Scan directory for audio files\n",
    "        wav_files = sorted([f for f in os.listdir(base_path) if f.endswith('.wav')])\n",
    "        print(f\"\\nFound {len(wav_files)} WAV files in directory\")\n",
    "        print(\"First few WAV files:\")\n",
    "        for f in wav_files[:5]:\n",
    "            print(f)\n",
    "            \n",
    "        # Match files with protocol entries\n",
    "        for wav_file in wav_files:\n",
    "            if wav_file in label_dict:\n",
    "                self.file_paths.append(os.path.join(base_path, wav_file))\n",
    "                self.labels.append(label_dict[wav_file])\n",
    "            else:\n",
    "                print(f\"Warning: No protocol entry for file {wav_file}\")\n",
    "        \n",
    "        # Print final statistics\n",
    "        genuine = self.labels.count(1)\n",
    "        spoofed = self.labels.count(0)\n",
    "        print(\"\\nDataset Statistics:\")\n",
    "        print(f\"Number of genuine samples: {genuine}\")\n",
    "        print(f\"Number of spoofed samples: {spoofed}\")\n",
    "        print(f\"Total files matched: {len(self.file_paths)}\")\n",
    "        \n",
    "        if len(self.file_paths) == 0:\n",
    "            raise RuntimeError(\"No files were matched between directory and protocol!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        try:\n",
    "            # Load and process audio\n",
    "            audio, sr = load_audio(file_path, self.sr)\n",
    "            if audio is None or sr is None:\n",
    "                # Create valid dummy tensor instead of zeros\n",
    "                dummy_input = torch.randn(3, 224, 224)  # Random noise is better than zeros\n",
    "                print(f\"Warning: Creating dummy tensor for {file_path}\")\n",
    "                return dummy_input, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "            # Process audio\n",
    "            audio = normalize_audio(audio)\n",
    "            audio = remove_gaussian_noise(audio, sr)\n",
    "            mel_spectrogram = extract_mel_spectrogram(audio, sr)\n",
    "            \n",
    "            # Ensure mel_spectrogram is not None and has correct shape\n",
    "            if mel_spectrogram is None or mel_spectrogram.size == 0:\n",
    "                dummy_input = torch.randn(3, 224, 224)\n",
    "                print(f\"Warning: Invalid mel spectrogram for {file_path}\")\n",
    "                return dummy_input, torch.tensor(label, dtype=torch.long)\n",
    "            \n",
    "            # Apply augmentation\n",
    "            if self.transform and 'train' in str(self.base_path).lower():\n",
    "                mel_spectrogram = spec_augment(mel_spectrogram)\n",
    "            \n",
    "            # Convert to image format\n",
    "            mel_spectrogram = np.stack([mel_spectrogram] * 3, axis=-1)\n",
    "            mel_spectrogram = Image.fromarray(np.uint8(mel_spectrogram))\n",
    "\n",
    "            if self.transform:\n",
    "                mel_spectrogram = self.transform(mel_spectrogram)\n",
    "            \n",
    "            # Ensure tensor is valid\n",
    "            if not isinstance(mel_spectrogram, torch.Tensor):\n",
    "                dummy_input = torch.randn(3, 224, 224)\n",
    "                print(f\"Warning: Transform failed for {file_path}\")\n",
    "                return dummy_input, torch.tensor(label, dtype=torch.long)\n",
    "            \n",
    "            return mel_spectrogram, torch.tensor(label, dtype=torch.long)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            dummy_input = torch.randn(3, 224, 224)\n",
    "            return dummy_input, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Audio processing functions\n",
    "def load_audio(file_path, sr=16000):\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=sr)\n",
    "        return audio, sr\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None, None\n",
    "# Test with a sample file to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Dataset class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, base_path, protocol_path, transform=None):\n",
    "        self.transform = transform\n",
    "        self.file_paths = []\n",
    "        # Add initialization code\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Add data loading code\n",
    "        pass\n",
    "# Initialize with small subset to test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Model architecture\n",
    "class EnhancedASVResNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "        # Add model components\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add forward pass\n",
    "        pass\n",
    "# Create small model instance to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training utilities\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    # Add training loop\n",
    "    return running_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TransformerBlock\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, mlp_ratio=4., dropout=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(dim * mlp_ratio), dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(*[self.norm1(x)]*3)[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# Define SETransformerBlock\n",
    "class SETransformerBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.se = SEBlock(channels, reduction)\n",
    "        self.transformer = TransformerBlock(channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        # SE attention\n",
    "        x = self.se(x)\n",
    "        # Transformer attention\n",
    "        x_trans = rearrange(x, 'b c h w -> (h w) b c')\n",
    "        x_trans = self.transformer(x_trans)\n",
    "        x_trans = rearrange(x_trans, '(h w) b c -> b c h w', h=h, w=w)\n",
    "        return x + x_trans\n",
    "\n",
    "# Define MultiScaleFeatureFusion\n",
    "class MultiScaleFeatureFusion(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels//4, 1),\n",
    "            nn.BatchNorm2d(channels//4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels//4, 3, padding=1, dilation=1),\n",
    "            nn.BatchNorm2d(channels//4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels//4, 3, padding=2, dilation=2),\n",
    "            nn.BatchNorm2d(channels//4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels//4, 3, padding=4, dilation=4),\n",
    "            nn.BatchNorm2d(channels//4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        x4 = self.branch4(x)\n",
    "        return self.fusion(torch.cat([x1, x2, x3, x4], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Main training loop\n",
    "def main():\n",
    "    # Initialize components\n",
    "    model = EnhancedASVResNet().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss = validate(model, val_loader, criterion)\n",
    "        print(f\"Epoch {epoch}: Train loss={train_loss:.4f}, Val loss={val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Execute training\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "name": "resnet_notebook"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
